---
title: Data Mining
tags: [数据挖掘引论，系统学习]
categories: [短期支线，大学课程，大三课程]
description: 数据挖掘的基本概念总结
date: 2023-04-02 13:08:37
---

# 数据挖掘

## 引论

### 什么是数据挖掘

* **数据中的知识发现**（Knowledge discover in data）
* **数据挖掘的基本步骤：**<a name='作用'></a>
  1. 数据清理： 删除噪声和不一致的数据
  2. 数据集成： 将多个数据源组合在一起
  3. 数据选择： 从数据库中提取出与分析任务相关的数据
  4. 数据变换： 通过汇总或聚集操作，将数据变换和统一为适合挖掘的形式
  5. 数据挖掘： 用智能方法提取数据模式
  6. 模式评估： 用某种兴趣度度量，识别真正有趣的模式
  7. 知识表示： 使用可视化和知识表示技术，向用户提供挖掘的知识

### 挖掘什么类型的数据

* 数据库数据
* 数据仓库
* 事务数据

### 挖掘什么类型的模式

* 特征化与区分 

* 频繁模式，关联和相关性

* 预测分析：分类与回归

* 聚类分析

* 离群点分析

* 所有模式都是有趣的么？

  **有趣：**非平凡的，蕴含的，潜在有用的，先前未知的

  **度量：**

  - **客观度量**
    - 支持度
    - 置信度
  - **主观度量**

### 使用什么技术

* 统计学
* 机器学习
  - 监督学习
  - 无监督学习
  - 半监督学习
  - 主动学习



## 认识数据

### 数据对象与属性类型

* **属性：**是一个数据字段，表示数据对象的一个特征。
* **标称属性**： 一些符号或事物的名称。被视为**分类的，无序的，枚举的**。
  * **二元属性**：只有两个状态0或1，0表示不出现，1表示出现
    - **对称二元属性**：两种状态具有同等价值，如男性，女性
    - **非对称二元属性**：两种状态价值不同，比如病毒检测结果阳性更具价值
* **序数属性**：值域元素之间存在有意义的序或者秩，但是排序相邻的元素之间的差值是未知的。**如**：大杯，中杯，小杯，我们并不知道大杯比中杯大多少，只知道这个排序。
* **数值属性**：定量的属性，可以用整数或者实数表示，是可以**区间标度的或比率标度的**
  - **区间标度**：例如温度，可以说一个温度比另外一个温度高多少度。但是华氏度和摄氏度都不能说一个温度比另外一个温度高多少倍。**它们只是有相同的单位尺度，但没有绝对的零点**
  - **比率标度**：具有固定零点的数值属性。**如**：公司员工数量，货币量等等
* **离散属性与连续属性**



### 数据的基本统计描述

* **中心度量趋势**

  * **均值**：

    * **普通均值**![image-20230218135005604](https://cdn.jsdelivr.net/gh/fushunhesir/blog-images@main/imgs/均值.png)

    * **加权均值**![image-20230218135051127](https://cdn.jsdelivr.net/gh/fushunhesir/blog-images@main/imgs/加权平均.png)

    * **截尾均值**：去掉极高和极低的数据的均值

  * **中位数**：

    * 奇数个数据：$pos=\frac{n+1}{2}$

    * 偶数个数据：$pos=[\frac{n}{2}, \frac{n}{2}+1]$

    * **近似中位数**：

      ![image-20230218140402849](https://cdn.jsdelivr.net/gh/fushunhesir/blog-images@main/imgs/近似中位数.png)

  * **众数**

    * 一个数据集中可能有多个众数
    * **如果每个数据仅出现一次，那么该数据集没有众数**

  * **中列数**：一个数据集最大值和最小值的平均值

  * **数据分布**：

    * **对称**：中位数等于众数
    * **非对称**：
      1. **正倾斜**：中位数大于众数
      2. **负倾斜**：中位数小于众数

* **度量数据散布**

  * **极差**：数据集最大值和最小值的差值

  * **四分位数**：用3个数据点将数据集划分为大小相等的4个子集，这三个数据点为**四分位数**。第二个四分位数为中位数。

  * **四分位数极差**：第三个四分位数减第一个四分位数，即：$IQR=Q_3-Q_1$，表示数据中间一半覆盖的范围。

  * **盒图**：

    * 盒体上边界为： $Q_3$, 下边界为：$Q_1$
    * 上胡须延长至：$max(1.5*IQR + Q_3, 极大值)$，下胡须延长至：$min(Q_1-1.5IQR, 极小值)$
    * **离散点**：超过四分位数1.5*IQR的数据，单独画出。

  * **方差和标准差**：

    ![image-20230218143341411](https://cdn.jsdelivr.net/gh/fushunhesir/blog-images@main/imgs/方差.png)

* **基本统计描述的图形表示**

  * 分位数图
    * **纵轴**：属性的数据范围
    * **横轴**：$f=\frac{i-0.5}{N}$

* 分位数-分位数图

  * 直方图

* 散点图

### 数据可视化

* 数据可视化的意义：通过图形清晰有效的表示数据

### 度量数据的相似性和相异性

* **数据矩阵和相异性矩阵**

  * **数据矩阵**：$n*p$ 矩阵，n个对象，p个属性

  * **相异性矩阵**：$n*n$ 矩阵， **其中元素代表相异性度量 $d(i,j)$**

  * **相似性度量**：$sim(i,j)=1-d(i,j)$

  * **标称属性的邻近性度量**：$d(i,j) = \frac{p-m}{p}$, 其中p为标称属性个数，m为两对象在相同的标称属性上值相同的个数。

  * **二元属性的邻近性度量**

    * 对称属性：$d(i,j)=\frac{r+s}{q+r+s+t}$
    * 非对称属性：$d(i,j)=\frac{r+s}{q+r+s}$, 因为负负属性不重要，所以直接去掉

  * **数值属性的邻近性度量：闵可夫斯基距离**

    * 欧几里得距离：又称**L2范数**
    * 曼哈顿距离：又称**L1范数**
    * 上确界距离：$d(i,j)=max\lvert x_{if}-x_{jf}\rvert$, 即最大差值，也称**一致范数**

  * **序数属性的邻近性度量**：$z_{if}=\frac{r_{if}-1}{M_{if}-1}$, 将序数属性转化为数值属性进行计算

  * **混合类型属性的相异性**

    $d(i,j)=\frac{\sum_{f=1}^pa_{ij}^{(f)}d_{ij}^{(f)}}{\sum_{f=1}^pa_{ij}^{(f)}}$

    * 只要有一个对象的属性值$x_{ij}$缺失，则$a_{ij}=0$,否则$a_{ij}=1$
    * f为数值属性，则：$d_{ij}^{(f)}=\frac{\lvert x_{if}-x_{jf}\rvert}{max_hx_{hf}-min_hx_{hf}}$
    * f是标称或二元，则若$x_{if}=x_{jf}$,则$d_{ij}^{(f)}=0$，反之为1
    * 序数的就转化为数值属性处理即可

  * **余弦相似性**：**针对文档等稀疏的词向量**



## 数据预处理

### 数据预处理概述

* **数据质量**：为什么要数据预处理
  * **准确性**： 数据是没有错误的
  * **完整性**：数据属性值是完整的
  * **一致性**：数据记录之间是没有冲突的
  * **时效性**：时间的影响
  * **可信性**：挖掘者是否信任该数据
  * **可解释性**：能知道怎么解释这个数据的意义
* **数据预处理的主要任务**（[作用](#作用)）
  * **数据清理**
  * **数据集成**
  * **数据规约**
  * **数据变换**

### 数据清理

* **缺失值**

  * 忽略整个元组
  * 人工填写
  * 全局常量填写
  * 中心度量填写
  * 同一类元组的中位数或均值填充(**当数据是倾斜的选择中位数**)
  * 最可能值填写：通过回归，贝叶斯，决策树等方法

* **噪声**

  * **分箱**
    * 箱均值光滑
    * 箱中位数光滑
    * 箱边界光滑
  * **回归**：通过函数拟合
  * **离群点分析**：聚类分析等方法

* **数据清理作为一个过程**

  * 偏差检测
    * 元数据：属性的数据类型和定义域，均值，中位数，倾斜还是对称
    * 字段过载
    * **唯一性规则**：值域没有重复元素
    * **连续性规则**
    * **空值规则**

* **数据集成**

  * **实体识别问题**：两个数据源进行属性匹配，**如：cus_id与cus_num**

  * **冗余和相关性分析**

    * **标称数据采用卡方检验**：

      ![image-20230218164941969](https://cdn.jsdelivr.net/gh/fushunhesir/blog-images@main/imgs/卡方检验.png)

    * **数值属性的相关系数**

      ![image-20230218165230847](https://cdn.jsdelivr.net/gh/fushunhesir/blog-images@main/imgs/相关系数.png)

    * **数值属性协方差**：$Cov(A,B)=E(AB)-E(A)E(B)$

### 数据规约

* **数据规约概述**
  * **维规约**：减少属性个数，**如：小波变换，主成分分析**
  * **数量规约**：用较小的数据代替元数据，**如：直方图，聚类，抽样**
  * **数据压缩**
    * 有损：近似重构原数据
    * 无损：重构后不损失信息



### 数据变换与数据离散化

* **数据变换策略概述**

  * 光滑：去掉噪声，**分箱，回归，聚类**
  * 属性构造：根据已有属性，**构造新的属性**
  * 聚集：对数据进行汇总和聚集，**如根据月销售量聚集成年销售量**
  * 规范化： **将数据缩放进一个小区间内**
  * 离散化：**数值属性的原始值用区间或标签替代**
    * 分箱
    * 直方图
    * 聚类、决策树、相关分析
  * 有标称属性产生概念分层

* **规范化数据**

  * **最大最小规范化**：映射到目标区间。

    $v^{'}_i=\frac{v_i-min_A}{max_A-min_A}*(new\_max_A-new\_min_A)+new\_min_A$

  * **z分数规范化**：$v^{'}_i=\frac{v_i-\bar{A}}{\sigma_A}$

  * **小数标定规范化**：$v^{'}_i=\frac{v_i}{10^j}$，其中j为使v最大**绝对值**小于1的最小整数



## 挖掘频繁模式、关联和相关性

性质是什么啊。反正apple算法都是重点啊，它步骤是什么啊？如何得到关联规则啊？这个这个他的优缺点啊，然后这个这个啊改进的一些思路、一些基本思想，有什么缺陷啊等等等等等说法

这个是呃第六章啊最重要的算法，没有之一啊，然后其次是f算法，它跟apple算法的区别，它的思想、它的步骤，对吧？

这个也是有给大家布置的作业的啊优缺点啊，特别是什么？一个做这个地方我们有一个非常重要的概念，可伸缩性，对吧？所以上测试我们也有这个题目啊，可伸缩性啊。好，然后呢就是使用垂直模式挖掘的好处啊，以及使用锤子模式那种方式很简单，对吧？

我们一共就两页PPT啊，很简单很容易掌握，对吧？

什么时候用垂直模式挖掘它有啥好处？好，以及第六章最后一个部分模式评估，我们给大家介绍五个兴趣度度量啊，哪些是零不变的。什么叫零不变性啊？啊，不平衡笔是用来干嘛的。对吧？然后学会能够用他们来评估。好，这个是第六章、第七章。

### **基本概念**

* **频繁模式**：频繁出现在数据集中的模式(项集，子序列，子结构)
* **关联规则**：$A\Rightarrow B[support=x\%;confidence=y\%]$
* **支持度**：表示所有事务中A和B同时出现的占比，即：$P(A\cup B)$
* **置信度**：表示事务中A和B同时出现的次数/事务中出现A的次数，即：$P(B|A)$
* **强规则**：同时满足最小支持度阈值和最小置信度阈值。
* **绝对支持度**：由频数作为支持度
* **频繁项集**：出现次数超过最小支持度计数
* **挖掘关联规则的步骤**
  * 找出所有频繁项集
  * 由频繁项集产生强关联规则
* **闭**：没有真超集能够和他有相同的支持度计数
* **闭频繁项集**：闭+频繁
* **极大频繁项集**：没有超集是频繁项集
* **可伸缩性**：==随着数据量的增加，数据挖掘算法的运行时间必须是可遇记的，短的和可以被接受的==。

### **Aprior算法**

* **先验性质**
  * ==频繁项集的所有非空子集也一定是频繁的==
  * ==非频繁项集的的超集也一定是非频繁的==
* **步骤**
  * **连接步**
  * **剪枝步**
* ==**缺陷**：==
  * 仍然可能产生大量候选集
  * 可能重复扫描数据库
  * 候选支持度计数工作量繁重

* **由频繁项集产生关联规则**
  * 计算置信度即可
* **提高Apriori算法的效率**
  * 基于散列，桶计数小于最小支持度计数的直接剔除
  * 事务压缩
  * 划分
  * 抽样

### 频繁模式增长

* 对于挖掘长的频繁模式和短的频繁模式，它都具有有效性和可伸缩性，并且比Apriori算法快一个数量级。
* 不产生候选项
* 压缩数据库
* 不重复扫描整个数据库

### 哪些模式是有趣的

* **提升度**：$lift(A,B)=\frac{P(A\cup B)}{P(A)P(B)}$
  * 若提升度大于1，则两者正相关
  * 若提升度小于1，则两者负相关
  * 若提升度等于1，则两者相互独立
* **卡方相关性分析**：$X^2=\sum\frac{(观测值-期望值)^2}{期望值}，期望值=\frac{count(a)*count(b)}{n}$
* **全置信度**：$all\_conf(A,B)=\frac{sup(A\cup B)}{max\{sup(A),suo(B)\}}$
* **最大置信度**：$max\_conf(A,B)=max\{P(A|B),P(B|A)\}$
* **Kulczynski度量**：$Kulc(A,B)=\frac{1}{2}P(A|B)+P(B|A)$
* **余弦度量**：$cosine(A,B)=\frac{P(A\cup B)}{\sqrt{P(A)P(B)}}=\sqrt{P(A|B)*P(B|A)}$

==所有以上度量都满足0～1，且值越大联系越紧密==

* **零事务**：不包含被考察的项集的事务。
* ==除了提升度和卡方相关分析外，其余度量都是零不变度量，因为它们不受零事务影响==

## 高级模式挖掘

### 基本概念

* ==**负模式**：如果项集X和Y都是频繁的，但很少一起出现，则项集X与Y是负相关的，且$X\cup Y$为负模式==
* ==**稀有模式**：支持度低于用户指定的支持度阈值的模式==
* 

## 分类：基本概念

有监督学习，无监督学习概念

分类，数字预测概念

分类的步骤

决策树和朴素贝叶斯

过拟合

### 基本概念

* **预测问题**
  * **数值预测**：回归分析是最常用的方法，**例**：预测一位顾客将在购物期间花多少钱。**构造的模型预测一个连续值函数或有序值**，这种模型称为**预测器**
  * **分类**：**构造模型或者分类器来预测类标号**
    * **学习阶段**：构建分类模型
    * **分类阶段**：使用模型预测给定数据的类标号
    * **类标号属性**：离散、无序
* **监督学习**：分类器的学习在被告知每个训练元组属于哪个类的“监督”下学习
* **无监督学习**：每个元组的类标号是未知的，且要学习的类的个数事先也可能是未知的
* **过拟合**：
* **准确率**：分类器正确分类的测试集元组所占的百分比
* **正元组**：感兴趣的主要类的元组
* **负元组**：其他元组

### 决策树归纳

* **基本算法**：

  * **三个参数**
    * **数据分区**：元组集合
    * **属性列表**：元组属性的列表
    * **属性选择度量**：基尼指数、信息增益等
  * 树从单个节点N开始
  * **如果**D中所有元组都同一类，则节点N变成树叶，**并用该类标记它**
  * **否则**调用属性选择度量来指定**分裂属性**和**分裂点**
  * 节点N利用**分裂准则**标记作为节点上测试，对于分裂准则的每个输出，生成一个分支，分支中的元素由D根据分裂准则划分生成
    * **属性A是离散值**：划分准则就是某几个离散值
    * **属性A是连续值**：根据分裂点划分为两个部分，生成两个分支
    * **属性A是离散值却必须生成二叉树**：分为yes，no集合
  * 对于结果分区$D_j$递归调用函数，生成决策树。
  * **终止条件**：
    * 分区中所有元组都属于同一类
    * 没有剩余属性可供划分元组，**使用多数表决**
    * 分支没有元组，**使用D中的多数类创建一个树叶**
  * 返回决策树

* **属性选择度量**

  * **信息增益**——越大越好

    * **信息熵**：$info(D)=-\sum_{i=1}^mp_ilog_2(p_i)$

    * **通过属性划分后的信息增益**：$Gain(A)=info(D)-info_A(D)$

      **解释**：通过对A的分裂后，未知信息减少了多少

    * **划分后的信息总量**：$info_A(D)=\sum_{i=1}^v\frac{|D_i|}{|D|}*info(D_j)$

    * **如果处理的是连续的值那么需要选择分裂点，排序后人为划分**：分裂点是两个连续值的中间值，分裂的分支数一定为**2**

    * **缺点**：偏向选择包含大量值的属性

  * **增益率**——越大越好

    * $SplitInfo_A(D)=-\sum_{j=1}^v\frac{|D_j|}{|D|}*log_2(\frac{|D_j|}{|D|})$

      **解释**：以划分属性分类来计算信息熵，原来的$info(D)$是以最终的类别

    * $GrainRate(A)=\frac{Grian(A)}{SplitInfo_A(D)}$

  * **基尼指数**——越小越好

    * $Gini(D)=1-\sum_{i=1}^mp_i^2$

      **代表不纯度**；$p_i$代表划分子集中属于最终类的百分比；

    * **离散属性**：需要将属性划分为2个子集，**考虑所有划分**

    * **连续属性：**处理方法和信息增益一致

    * $Gini_A(D)=\frac{|D_1|}{|D|}Gini(D_i)+\frac{D_2}{D}Gini(D_2)$

      $\delta Gini(A)=Gini(D)-Gini_A(D)$

    * **要求$Gini_A(D)$越小越好，$\delta Gini(A)$越大越好**

* **树剪枝**

  * **先剪枝**：可以使用**统计显著性、信息增益、基尼指数等度量划分的优劣，如果划分节点时低于阈值就停止划分**
  * **后剪枝**：完全生长的树剪去子树。
    * **CART的代价复杂度剪枝算法：**用剪枝集评估代价复杂度，最小化代价复杂度为目标
    * **C4.5悲观剪枝**：不使用剪枝集而使用训练集，基于认为训练集评估准确率或错误率过于乐观

* **可伸缩性与决策树归纳**

  * **RainForest**
  * **BOAT**：树构造的自助乐观算法 **思想：**取样，精度换速度

### 贝叶斯分类方法

* **贝叶斯定理**
  * **后验概率**：已知某些关于这件事条件，发生这事情的概率
  * **先验概率**：一切未知的情况下，这件事发生的概率
  * $P(H|X)=\frac{P(HX)}{p(X)}=\frac{P(X|H)P(H)}{P(X)}$
* **朴素贝叶斯分类**
  * 使后验概率最大化
  * **类条件独立假设**
  * **如果为连续值属性**：$p(x_k|C_i)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
  * **步骤**
    * **计算每个分类的先验概率**
    * 计算不同分类下各个属性的后验概率
    * **计算给定条件的后验概率**
    * **计算先验概率与给定条件后验概率的乘积，选择最大的分类作为最终分类**
  * **若存在零概率**：拉普拉斯校准，在相应属性每个值各添加一个元组

### 模型评估与选择

* **评估分类器性能的度量**
  * **准确率**：$accuracy=\frac{TP+TN}{P+N}$
  * **错误率**：$error\quad rate=\frac{FP+FN}{P+N}$
  * **灵敏性,召回率**：$sensitivity=\frac{TP}{P}$
  * **特效性**：$specificity=\frac{TN}{N}$
  * **F度量**：$F=\frac{2*precision*recall}{precision+recall}$
  * $F_\beta$**度量**：$F_\beta=\frac{(1+\beta^2)*precision*recall}{\beta^2*precision+recall}$
* **保持方法和随机二次抽样**
* **交叉验证**

## 分类：高级方法

### 基本概念

* ==**后向传播**：即通过类实例标号，不断修正参数，从输出层到第一个隐藏层，最终使参数收敛==
* ==**贝叶斯信念网络**：解决朴素贝叶斯类条件独立的问题==
* **支持向量机**：通过非线性映射，将数据映射到更高维，搜索最佳分离超平面

## 聚类分析：基本概念和方法

### 基本概念

**聚类**：把数据对象划分为多个组或簇的过程，使得簇内的对象具有很高的相似性，但与其他簇中的对象很不相似

### 聚类分析

* **对聚类分析的要求**

  * 可伸缩性
  * 处理不同属性类型的能力
  * 发现任意形状的簇
  * 对于确定输入参数的领域知识的要求
  * 处理噪声数据的能力
  * 增量聚类和对输入次序不敏感
  * 聚类高维数据的能力
  * 基于约束的聚类
  * **可解释性和可用性**
  * 划分准则
  * 簇的分离性
  * **相似性度量**
  * 聚类空间

* **基本聚类方法概述**

  * **划分方法**

    * 给定一个n个对象的集合，划分方法构建数据的k个分区，其中每个分区代表一个簇，$k\leq n$

      **大部分划分是基于距离的**，采用**迭代重定位的方法**

      * **迭代重定位**：把一个对象移动到另外一个组来改进划分。

    * **缺点**：计算量极大，相当于枚举，**基于距离的方法只能发现球状簇**

  * **层次方法**

    * **凝聚**：自底向上，最初将每个对象单独作为一个组，然后逐次合并相近的组或对象直到所有的组合并为一个组，或者满足某个终止条件
    * **分裂**：自顶向下，开始将所有对象放入一个组，每次划分为一个更小的簇，或者满足某个终止条件
    * **缺点**：一个步骤完成，就不能撤销

  * **基于密度方法**

    * **思想**：只要邻域中的密度超过某个阈值就继续增长

  * **基于网格**

### 划分方法

* **K-means：基于形心的技术**
  * **工作流程**
    1. 首先随机地选择k个对象，每个对象代表一个簇的初始均值或中心
    2. 对剩下的每个对象，根据其与各个簇中心的欧式距离，分配到最近的簇
    3. 利用新分配到的对象，重新计算新的中心不断迭代
  * **缺点**
    * 不能保证k-means**收敛于全局最优解**，在实践中，可能会选择不同的初始簇中心运行k-means
    * 需要预先定义k值，即多少个簇
    * 严重受离群点影响
* **K-中心点：一种基于代表对象的技术**
  * **思想**：不用对象的均值作为参照点，**而是用一个实际对象代表簇**
  * **误差标准**：$E=\sum_{i=1}^k\sum_{p\in C_j}dist(p,o_i)$

### 层次方法

* **凝聚的与分类的层次聚类**
  * 凝聚
    * **每个簇都用簇中所有对象代表**
    * 两个簇的相似度用不同簇中最近的数据点对的相似度来度量
* **算法方法的距离度量**
  * **最小距离**：两个簇中距离最近的两个点的距离
    * **如果最近的两个簇之间的距离超过阈值，聚类终止，则称其为单连接算法**
  * **最大距离**：两个簇中距离最远的两个点的距离
    * **如果当最近的两个簇之间的最大距离超过阈值，聚类终止，则称其为全来接算法**
    * **如果真实的簇较为紧凑且大小近似相等**，**则这种方法将会产生高质量的簇，否则毫无意义**
  * **均值距离**：中心点的距离
  * **平均距离**：**计算两个簇中所有点组合的距离均值**
* **BIRCH：使用聚类特征树的多阶段聚类**
  * **应用场景**：大量数值数据聚类
  * **优点**：可伸缩性，可以撤销先前步骤的工作
* **Chameleon：使用动态建模的多阶段层次聚类**
  * **思想**：动态建模确定一对簇的相似度
  * **优点**：不用依赖于一个静态的，用户提供的模型，能够自适应
* **概率层次聚类**
  * **思想**：通过概率模型度量簇之间的距离，克服以上某些缺点

### 基于密度的方法

* **DBSCAN：一种基于高密度连通区域的基于密度的聚类**
  * **核心对象**：*邻域内的对象数量大于等于阈值*
  * **邻域密度**：邻域内的对象数度量
  * **直接密度可达**：该点在**核心对象**点邻域中
  * **密度可达**：*直接密度可达传递*，**出发点和中间点必须是核心对象**
  * **密度相连**：*两个中心能够从同一个点密度可达*

### 基于网格的方法

* **思想**：将对象空间量化为有限数目的单元，这些单元形成了网格结构，所有的聚类操作都在该结构上进行
* **优点**：处理速度快，处理速度进依赖于量化空间中每一维上的单元数

### 聚类评估

* 

## 高级聚类分析

## 离群点检测

什么是离终点啊。离群点和其他的一些相似概念有什么区别啊？离群点点的定义啊，经典的三种离群点的分类啊，离群点检测是个什么概念啊？这样一些啊。

然后呢就是四种离群点检测的方法，他们的原理，他们的假设，特别是他们的假设是什么？原理是什么？特点是什么啊，优缺点是什么啊，然后有监督、无监督从另外一个角度来划分，有监督、无监督、半监督这种离群点检测啊，他们的概念、特点啊是什么啊？

啊，优势。优缺点，优势是什么？适用于什么样的场景。对吧？啊，然后最后就是对于高危的触点它的挑战是什么？它的困难是什么？啊，基本思路是什么啊？挖掘情景的清点和集体的景点啊，他的思路是什么啊。这个这个对困难在什么地方？对吧？

### 基本概念

* **离群点**：显著不同于其他数据对象的数据对象

### 离群点和离群点分析

* **离群点的类型**
  * **全局离群点**
    * 显著的偏离数据中的其他对象
  * **情景离群点**
    * 关于对象的特定情景，显著地偏离其他对象。 比如夏天-1度
  * **集体离群点**
    * 单个点或许不是离群点，**但是这群点的集合是异常点**
* **离群点检测方法**
  * **监督半监督无监督方法**
    * 监督方法：专家标记基础数据的样本
      * 缺点：
        1. 离群点总体数量少，专家标记样本可能不足以代表离群点分
    * **无监督方法**
      * **假设**：正常对象在某种程度上是聚类的
      * **缺点**：不属于簇的可能是噪声而不是离群点，开销大
    * **半监督方法**

### 高维数据中的离群点检测

*  **挑战**
   * **离群点的解释**
   * **数据的稀疏性**
   * **数据子空间**
   * **关于维度的可伸缩性**
   * 

# Question

## 数据挖掘的功能是什么

## 数据挖掘的主要问题和挑战是什么

## 各个图的优缺点，适用场景是什么

## 数据可视化的意义

## 协方差等于0可能相关也可能无关

## 可伸缩性是什么

## 垂直模式挖掘的好处，什么时候用
